Things I learned about today:
I started my course in machine learning, and learned about the two segments of machine learning-
supervised and unsupervised learning.

Supervised learning is when you have a set of data that you know the answer to, and you want to find a way 
to predict the answer to new data. Some main types of supervised learning are classification and regression. 
Classification is when the target variable is a category, and regression is when the target variable is a number. 
For example when given data on the different characterisitics of a cancer cell, we would use classification in 
order to classify the cell into a certain predetermined catergory. However for regression, we are not classifying 
our end result into a category, but rather we are returning a number, of which infinite options are avaolable. 
When finding a correlation between square footage, location, and price of a house, we would use a regression to 
find the price that correlates to the square footage and location provided. It is not returning a category of how
 expensive it is, but instead speciifc numbers that represents the price of the house.

Unsupervised learning is when you have a set of data that you don't know prelabel and have determined outputs
in your dataset, and you want to find a way to group the data. Some main types of unsupervised learning are 
clustering, dimensionality reduction, and anomaly detection. Clustering is when you take your inputs, and determine
which inputs are most closely linked. For example, when we get suggestions for similar news stories, the
suggestions are located in the same cluster that our unsupervised learning algorithm found for us. 
Dimensionality reduction is when you take a very large set of data, and reduce its dimension to make a much smaller,
more manageable set of data, without losing much (if any) of the data's macroinformtaion. An example of this would
be when you have a set of data that has 1000 different features, and you want to reduce it to 10 features. Anomaly
detection is when you have a set of data, and you want to find the data that is most different from the rest of
the data. For example, when you are trying to find credit card fraud, you would use anomaly detection to find
the credit card transactions that are most different from the rest of the transactions.

Starting to learn about Linear Regressions- Regression models predicts numbers
(Fitting a straight line to a data set, and using that line to predict future values)

Terminology to know-
Training Set- data used to train our models (input features, and output targets (right answers that the model learns from))
Test Data- data used to test our models

Input Variables- usually denoted by "x", also known as a feature, or input feature (if multiple input variables then
denoted by "x1", "x2", etc.)
Output Variables- usually denoted by "y", also known as a target variable, or output feature
m - Total number of training examples
n - Number of input variables
(x,y)- Single Training example
(x^(i),y^(i))- ith training example (index of the training set)
i - specific row in the training set

Flowchart of a machine learning algorithm-
Training Set-> Learning Algorithm-> f (hypothesis/function/model (takes input x, and produces predicition (天)))->
Predictions (estimated value of y = 天, the true value is y)

How to represent f/our function-
f_(w,b)(x)=wx+b
w- weight (slope of the line)
b- bias (y-intercept of the line)
The function is making predicitons on y using a linear function of x, and the parameters are w and b
We use linear functions to keep our model simple, and to make it easier to train our model

Linear Regression with one variable/ Univariate linear regression- 
f_(w,b)(x)=wx+b - single feature x/ one variable x
f_(w,b)(x)=w1x1+w2x2+...+wnxn+b - multiple features x/ multiple variables x

In order to make any linear regression work, we need to create a cost function
Cost Function- measures the accuracy of our hypothesis function
We want to minimize the cost function, and we do this by changing the parameters w and b

天(i) = f_(w,b)(x^(i)) = wx^(i)+b

To determine how well our hypothesis function works, we need to create a cost function
To find the cost function, we need to find the difference between the predicted value and the actual value

Standard most common cost function formula- (Squared Error Cost Function)
J(w,b) = Summation from i=1 to m of (((天^i)-y^i)^2)/2m                   (2m to make later calculations easier)
where m is the number of training examples





