Today I created a simple algorithm from scratch on finding the k-nearest neighbors to a given point.
I used the euclidean distance formula to find the distance between the given point and all the other points in the
dataset. I then sorted the distances and returned the k nearest neighbors. I then used the most_common function
in order to find the most common type of class in the k nearest neighbors. I then returned the most common class.

This is useful obviously for classification problems, but can also be used for regression problems. For example,
if I did the same thing, but instead of predicting the class, I returned this point as an average of the k-nearest
neighbors, then I would be using this algorithm for regression problems.

I also created a function that would return the accuracy of the algorithm. I did this by comparing the predicted
class to the actual class and then dividing the number of correct predictions by the total number of predictions.

I then tested the algorithm on the iris dataset and got an accuracy of 96.67%.


The next thing I started learning about was how to do linear regressions with multiple variables.

Multiple Feature Linear Regression notes
Notation:
x_j = the jth feature (for housing example, size in feet, number of bedrooms, number of floors, age of 
house... would be labeled, x_1, x_2, x_3, x_4)

n = number of features

x^i = features of the ith training example

x_j^i = value of feature i in the jth training example

New model:

f_w,b(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n +b

w = vector of weights [w1, w2, w3, ..., wn]
x = vector of features [x1, x2, x3, ..., xn]

so our new model is a multiple linear regression where
f_w,b(x) = w*x + b (where * is the dot product)

In order to make our multiple linear regressions run faster, we need to use vectorization. This is where we
use numpy arrays and dot products as they are much more efficent than for loops and not using vectorization.

Converting from our old notation to new notation:

w = [w_0, w_1, w_2, ..., w_n]
f_w,b(x) = w*x + b (where * is the dot product)
Cost function - J(w,b)
Gradient Descent - minimize J(w,b)
w_j = w_j - alpha * partial derivative of J(w,b) with respect to w_j
b = b - alpha * partial derivative of J(w,b) with respect to b
where partial derivative of J(w,b) with respect to w_j = 1/m * sum from i=1 to m of (f_w,b(vector(x)^i) - y^i) * x_j^i
and partial derivative of J(w,b) with respect to b = 1/m * sum from i=1 to m of (f_w,b(vector(x)^i) - y^i)


An alternative to gradient descent is the normal equation. Only works for linear regressions.
It solves for w and b without interations.
However, it is slow if n is very large (n > 10,000), and does not generalize to other models.
Should only be used by machine learning libraries and not by programmers.



